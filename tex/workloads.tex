\section{DDC Traffic Characterization}
\label{sec:workloads}

In the previous section, we modeled the network as a pipe of
fixed latency and bandwidth, ignoring the impact of network effects 
such as congestion. We now remove this limitation. 
However, the impact of network congestion depends greatly
on the input traffic workload. Hence, in this section, we first try to
characterize traffic workloads under disaggregation.
We describe our methodology for this in \S\ref{ssec:method1} and then present our results in \S\ref{ssec:flc}.

\subsection{Methodology}
\label{ssec:method1} 

Our overall approach to generating traffic workloads is to collect traces 
of disk, memory, and network accesses when running applications on 
existing cluster architectures and then translate these traces into the 
network traffic that would have resulted had these applications been 
running over a disaggregated cluster. 
This involves two steps: (i) collecting traces of memory, disk and network 
access (on an existing server cluster) and, (ii) mapping these accesses to network flows (on a model of a disaggregated cluster).

\paragraphb{Traces}
We use the same experimental setup as in \S\ref{ssec:rmethod}, including the applications, datasets, and EC2 cluster (with VPC enabled). 
We collect traces of: 
\vspace{-0.5em}
\begin{itemize}[leftmargin=*]
\itemsep0em
\item \emph{`remote' memory accesses:} using our instrumentation that intercepts 
page faults as described in \S\ref{ssec:rmethod}; we record the 
request size in number of pages and the timestamp for each request.
%into the server's portion of the main memory that we've identified as ``remote memory''
\item \emph{network accesses:} we capture inter-machine traffic using {\tt tcpdump}~\cite{tcpdump}. {\tt tcpdump} gives us a packet-level log containing the five tuple for each packet. We treat all packets with the same five-tuple (protocol ID, source and destination address/port) as a single flow{\footnote{Applications choose one of the source ports randomly to establish a connection to the destination for each request. Given the large number of available ports, we assume that multiple connections between a pair of servers do not share the same port.  
One exception is memcached which uses long-lived connections (all packets between a client-server pair have the same five-tuple). In this case, we simply assumed each key-value pair lookup to be one flow.}}.
\item \emph{disk accesses:} we record application-level disk accesses using the {\tt blktrace} utility, which records a collection of (start\_block\_address, \#blocks, timestamp) tuples for each disk access request. Note that a single application-level file request may generate multiple such tuples since the data is not necessarily stored on consecutive disk blocks. Since {\tt blktrace} records accesses at the granularity of disk blocks, we must identify which tuples correspond to the same application-level request. This could be done by instrumenting the application; however, this is not only complex but also would potentially interfere with the application's performance. Instead, we simply assume that all tuples with timestamps within $50\mu$s correspond to the same application-level request. 
%The only difference is that we enabled the Virtual Private Network (VPC) in our cluster, ensuring no interference with other Amazon EC2 machines. 
\end{itemize} 

\cut{

\paragraphb{Capturing traffic in server-centric datacenters}

\paragraphb{Capturing memory and disk accesses}
At each individual server, we capture two types of traffic. First, application-level memory accesses using SIT (\S\ref{ssec:rmethod}) --- the request % ($75\%$ of main memory at the server since we use CPU local cache size to be $25\%$ (\S\ref{sec:requirements})). 

}
%
\begin{figure*}
  \centering
  \renewcommand{\thesubfigure}{(a)}
  \subfigure[Flow size distribution (\dis)]{
    \includegraphics[width = 2in]{img/graph1_sizedist_dis} 
	\label{fig:fsd}
  }
%\hspace{0.05in}
  \renewcommand{\thesubfigure}{(c)}
  \subfigure[\#Flow in \dis and \pdis]{
    \includegraphics[width = 2in]{img/graph2_numflows}
	\label{fig:nof}
  }
  \renewcommand{\thesubfigure}{(e)}
  \subfigure[Temporal traffic distribution (\dis)]{
    \includegraphics[width = 2in]{img/graph7_temporaltraffic_dis_terasort}
	\label{fig:ttd}
  }  
  \renewcommand{\thesubfigure}{(b)}
  \subfigure[Flow size distribution (\pdis)]{
    \includegraphics[width = 2in]{img/graph1_sizedist_pdis}
	\label{fig:fsdp}
  }
  \renewcommand{\thesubfigure}{(d)}
  \subfigure[Traffic volume in \dis and \pdis]{
    \includegraphics[width = 2in]{img/graph5_trafficvolume}
	\label{fig:tv}
  }  
  \renewcommand{\thesubfigure}{(f)}
  \subfigure[Temporal traffic distribution (\pdis)]{
    \includegraphics[width = 2in]{img/graph7_temporaltraffic_pdis_terasort}
	\label{fig:ttdp}
  }  
  \caption{\small{Flow and network traffic characteristics in \dis. Note that the x-axis is log-scaled for (a) and (d); the y-axis is log-scaled for all figures. \rc{Traffic volume is for terasort application} \rc{remove wordcount, rename wordcount-hadoop -> wordcount}}}
  \label{fig:traffic}
\end{figure*}
%

\paragraphb{Mapping accesses to network flows in \dis}
Mapping the above traces to network traffic that would have resulted 
in a disaggregated cluster again involves two steps. First, we must 
define the resource blades that form our endpoints in the disaggregated cluster (i.e., map the resources in our server-based cluster into standalone blades in a disaggregated cluster) and then map the above traces to flows between these endpoints. 
We assume one server maps to one compute blade and one memory blade but consolidate disk resources from different servers into a smaller number of disk blades so as to compensate for the relatively low throughput to disk (doing otherwise would only decrease our network load); specifically, we 
assume that one server's disk resources map to $60\%$ of a standalone 
disk blade.\footnote{We arrived at this number by aiming to balance network and disk throughputs while maintaining sufficient redundancy.} 

Having defined our resource blades as above, we now define how traffic 
flows between them. 
All memory and disk accesses captured above are associated with a specific address in the corresponding CPU's global virtual address space. We assume this address space is uniformly partitioned across all memory and disk blades reflecting our assumption of distributed data placement (\S\ref{ssec:system}).  
%This address space is partitioned across five memory blades and three disk blades{\footnote{Some of the disk space was not utilized in our cluster.}} in \dis using range partitioning. 
%distributes data across memory or disk access in our EC2 cluster may now be mapped along multiple blades.

One subtlety remains. Consider the disk accesses at a server A in the original cluster: one might view all these disk accesses as corresponding to a flow between the compute blade corresponding A and the disk blade corresponding to A but in reality some of these disk accesses may have been issued by A's CPU in response to a request from a remote server B (\eg, due to a shuffle request). 
In the disaggregated cluster, this access should be treated as a network flow between \emph{B}'s compute blade and A's disk blade (i.e., the 
compute blade obtained by disaggregating server B and the disk blade that corresponds to disaggregating A).


%The memory and disk requests using SIT and {\tt blktrace} are captured locally at each individual server. Some of these requests correspond to requests originating at remote CPUs (\eg, shuffle traffic). 

To correctly attribute accesses to the CPU that originates the request, we match network and disk traces across the cluster -- e.g., matching the network traffic between B and A to the disk traffic at A -- using both the timestamps and volume of data transferred. 
%perform a timestamp-based matching between requests captured at each individual server, and inter-machine NIC traffic from {\tt tcpdump}. 
If a locally captured memory or disk access request matches a local flow in our {\tt tcpdump} traces, then it is assumed to be part of a remote read and is attributed to the remote endpoint of the network flow.
%(more than $85\%$, on an average, were correctly attributed in our evaluation). 
Otherwise, the memory/disk access is assumed to have originated from the local CPU. 
%generated locally. 

\cut{
\paragraphb{Data placement}

for the entire address space nodes for a 5-machine ec2 cluster --- this represents the fact that disk access is slower than memory access, and thus can support more resource per blade without becoming bandwidth constrained.
In particular, the address space for memory and disk on each individual server is partitioned into five and three parts, respectively, to match the number of resource blades. Each range is then mapped to one of the blades. 

\paragraphb{Transforming accesses into network flows}

The above methodology provides us the amount of data accessed and the corresponding timestamps for each pair of CPU-memory and CPU-disk blades in \dis (memory-disk blades to not communicate directly). For any pair of resource blades, we transform access requests into network flows by combining requests with timestamps within $50\mu$s into a single flow.  Finally, a read request on disk and memory blades is considered a flow with corresponding blade being the source and CPU being the destination; write requests are considered as flows in the opposite direction.
}

\subsection{Traffic Characterization} 
\label{ssec:flc}
We analyze the traffic characteristics in \dis, and contrast them against the traffic in existing server-centric datacenters. 
We note that network designs have long been guided by measurements/models of traffic workloads and hence the characterization we undertake is of value in itself (i.e., beyond  just capturing the impact of congestion).

\paragraphb{Flow-level characteristics} We consider three metrics that characterize traffic at the flow level.

\noindent \emph{(1) Flow sizes:} Figure~\ref{fig:fsd} and Figure~\ref{fig:fsdp} show the distribution of flow sizes with and without disaggregation respectively. We note first that the flow size distributions we observe without disaggregation closely matches the distributions reported in previous studies~\cite{srikanth, theo} validating our baseline test setup. 

We observe that disaggregation dramatically reduces the range of flow sizes: flow sizes are concentrated within a smaller [4KB, 40MB] range and the largest flow sizes are almost two orders of magnitude smaller than in existing clusters.  Intuitively, two factors contribute to the differences. First, as outlined in \S\ref{ssec:rmethod}, the granularity of remote traffic in \dis is either a page size ($4$KB) or a disk block size ($4$KB) which leads to a lack of very short flows. Second, distributed data placement in \dis (\S\ref{ssec:rmethod}) distributes long flows along multiple CPU-disk blade pairs, reducing the tail in flow size distribution.

\noindent \emph{(2) Number of flows:} 
Figure~\ref{fig:nof} shows that the number of flows with and without disaggregation. We see that disaggregation increases the number of flows by $3$--$4$ orders of magnitude (except for memcached, discussed below). This is not surprising since disaggregation adds remote disk and memory accesses to the network. Interestingly, disaggregation \emph{decreases} the number of flows that memcached generates.
%in a five server cluster running memcached, one out of five client requests is injected into the network (since the desired data is cached on a remote server). 
This is because, in \dis, we fetch data from remote memory at the granularity of a page and cache the same locally and hence some lookups that, without disaggregation, manifested as requests to 
remote servers are now served from the CPU's local memory.
%with $25\%$ local cache, CPU blades can respond to $25\%$ of the requests locally and only $75\%$ of the requests are going to be injected into the network.
%\rc{$\gets$ I got confused while writing this argument; probably unncessarily. Will come back later}

\noindent \emph{(3) Flow inter-arrival times: } Because job completion times in \dis remain comparable to that in current server-centric architectures (\S\ref{sec:requirements}) but the number of flows increase by orders of magnitude, we can expect that inter-flow arrival times in \dis will be several orders of magnitude smaller than in server-centric datacenters. 
%Smaller inter-arrival times combined with concentration of flows within a smaller flow size range ($4$KB flows dominating the flow size distribution for many applications) have the following implication in protocol design for \dis:
This suggests that centralized flow schedulers~\cite{fastpass} and `reactive' network controllers (i.e., that react to each new flow arrival)~\cite{nox} will face new scaling challenges; we return to evaluate this in \S\ref{sec:existing}.
%both in scalability and in meeting \dis application performance goals.

\paragraphb{Network-level traffic characteristics} 
\label{sssec:fctv}
Next, we consider network-wide traffic volume and distributions.

\noindent \emph{(1) Total traffic volume:}
Figure~\ref{fig:tv} compares the total traffic volume with and without disaggregation. As expected, traffic volume increases significantly in \dis due to (now) remote memory and disk accesses.
While it may appear surprising that the increase in traffic volume is not proportional to the increase in number of flows, this is simply because most of the ``new'' flows are memory flows and these contribute few bytes (approx. 4KB/flow) to the overall traffic volume. 
%that have very little contribution to the overall traffic volume.
With memcached, the traffic volume drops by almost $25\%$ for the same reason described above. 
%, precisely due to the effect that leads to reduction in number of flows.


\noindent \emph{(2) Temporal traffic distribution:}
Figure~\ref{fig:ttd} and Figure~\ref{fig:ttdp} show the temporal traffic distribution in \dis and in existing datacenters. We make two observations. 
First, traffic in \dis is more uniformly distributed across time. This is to be expected as network traffic now results from memory and disk accesses which occur throughout the job's lifetime  
as opposed to (for example) network traffic that is generated only during a specific 
phase of a job's lifetime (e.g., the shuffle phase in a map-reduce job).

%, with little or no traffic bursts. 
%This is because of an interesting effect caused by the data placement discussed in \S\ref{sec:summary}. In particular, the data placement in \dis allows a long memory or disk read to be spread across multiple smaller flows spanning several memory or disk blades. Intuitively, this has an affect akin to the task packing problem, where having multiple smaller tasks has a better packing compared to a single long task. That is, dividing a single long flow into multiple smaller flows ``packs'' into the network more efficiently, making traffic less bursty.

The second and perhaps more surprising observation is that, although the traffic volume in \dis is two to three orders of magnitude larger than in server-centric datacenters, the peak traffic does not change. 
This suggests that, an increase in \dis network bandwidth capacity is more likely to be driven by the need to reduce end-to-end network latency (e.g., our conclusion in  \S\ref{sec:requirements} of a 40Gbps requirement) than to support higher peak traffic volumes.

% \vspace{0.1in}
% \noindent
% While resource disaggreation does not require increase in network bandwidth capacity to handle increased traffic volume, the increase in bandwidth can be beneficial on other fronts (\eg, reducing end-to-end network latency, as dicussed in \S\ref{ssec:rtt}).


\noindent \emph{(3) Spatial traffic distribution:}
We plot the spatial distribution of traffic that results from disaggregation in Figure~\ref{fig:sd}. 
We see that the communication pattern is highly non-uniform between pairs of endpoints which is to be expected since only CPU blades communicate with memory or disk blades (i.e., some endpoints 
will never communicate). While this seems to suggest that uniform network designs based on (for example) fat-tree topologies that offer full bisection bandwidth between all endpoints are ill-suited to \dis, this is not the case because of our decision to mix different types of resource blades in each rack. This makes the non-uniformity we observe disappear when viewing the spatial distribution of communication at the rack level. 

\paragraphb{Recap} In summary, we find that disaggregation leads to significantly altered traffic workloads, changing many long-held assumptions such as a high skew in flow-size distributions and the importance of `elephant' flows. In the following section, we test existing network designs on these new workloads.


\cut{ 
Also note   traffic volume to the first memory blades than to memory blades farther rightwards and upwards in the heatmap. Digging deeper, we found that lower memory addresses were more popular in the application shown, terasort\footnote{results for other applications are in Appendix ~\ref{sec:appendix}}. As a result, the memory blade that lower addresses were mapped to in our range hashing experienced a greater traffic volume from CPUs. This suggests that depending on the application's data access patterns, a different address mapping may be needed. \rqc{say that: While in \pdis the traffic volume is spread over all source-destination pairs, in \dis a number of such pairs have no traffic between them --- that is, the traffic matrix changes from all-to-all to some-to-some.}
}

\begin{figure}[t]
  \centering
    \includegraphics[width = 2.3in]{img/graph6_trafficvolumeheatmap_dis_terasort} 
%	\label{fig:asdd}
%\hspace{0.05in}
%  \subfigure{
%    \includegraphics[width = 2.3in]{img/graph6_trafficvolumeheatmap_pdis_terasort}
%	\label{fig:sdds}
  }
  \caption{\small{Spatial traffic distribution in \dis for the terasort application. The x-axis represents senders and the y-axis receivers. The heatmap is a $13 \times 13$ matrix, for a `disaggregated' cluster with 5 cpu blades, 5 memory blades, and 3 disk blades.}}
  
  %A $n \times n$ matrix heat diagram, with cell $(i, j)$ having heat level corresponding to the traffic volume between source i and destination j. The same thing for \pdis.}}}
  \label{fig:sd}
\end{figure}

%---------------
%END OF USEFUL STUFF
%---------------

% Just as memcached exhibits a greater number of flows in \pdis, its traffic volume is greater in \dis. Our experiments with 25\% of memory in the local cache, so we expect that approximately this amount of request traffic in memcached will be served from the local cache in \dis and avoid the network entirely. Accordingly, we observe that the difference in traffic volume between \dis and \pdis is 24\%. The difference in the number of flows is more drastic because the flow distributions are different --- while memcached in \pdis has a bimodal distribution of request and response flows of approximate size 30 bytes and 10KB respectively, in \dis the distribution varies in the range [4KB, 19MB]. The longer tail in flow sizes for \dis causes the drastic disparity in number of flows seen in Figure~\ref{fig:nof} despite a relatively modest increase in traffic volume.

%\rc{This section along with figure 8 are slated for removal because we cannot count remote memory flows.}
%Next, we study the flow arrival times in \dis. Figure~\ref{fig:fat} shows the flow arrival time distribution for the six applications from Table~\ref{tab:workloads}. We make two interesting observations. First, the flow arrival time distribution is essentially independent of the application. This is because memory access traffic dominates all the applications, leading to similar flow arrival time distribution. Second, the flow arrival time distribution is significantly different from Poisson distribution, a commonly accepted approximation to workloads in \pdis. \rc{We should say something about inter-flow arrival times --- too small? not too small? any significant different compared to \pdis?}
%
%Figure~\ref{fig:fat} also shows the flow arrival time distribution in \pdis. As observed in several previous studies~\cite{srikanth, theo}, the distribution is very close to Poisson distribution providing a second order support to our evaluation methodology. In comparison to \pdis, the distribution is significantly different in \dis as discussed above.

%Second, across all the applications we evaluated, the flow size distribution is dominated by memory traffic (4KB flows) and at least $60\%$ of the flows are less than $100$KB. 
% 
%  results to discuss:
% \begin{itemize}
% \item For most applications, both the number of flows and the overall traffic volume in \dis are greater than the number of flows in \pdis. \rqc{\#flows increase by 3-4 orders of magnitude, but volume only by 2}
% \item \rqc{handling the increased traffic volume does not require 2 orders of magnitude more bandwidth --- peak traffic volume does not change}
% \item \rqc{rather non-intuitively, applications executing in-memory point queries require fewer flows and volume} 
% \item While in \pdis the traffic volume is spread over all source-destination pairs, in \dis a number of such pairs have no traffic between them --- that is, the traffic matrix changes from all-to-all to some-to-some.
% \item \rqc{stuff from yet-to-be-written 4.4}
% \end{itemize}
%



% We note that the traffic volume in \dis is not much larger than in \pdis. At the first glance, this may seem rather surprising given the increase in traffic due to intra-server traffic becoming remote. However, note that memory accesses are just $4$KB in size; thus, a significantly large number of memory access traffic will correspond to a single long flow. Moreover, reads and writes that are local are only increased by a factor of $2\times$.
%
%First, when compared to \pdis, the traffic in \dis is significantly well distributed across the source-destination pairs. Indeed, the \dis architecture allows distributing a single read/write traffic across multiple nodes spatially distributed across the network. Second, the overall spatial distribution in \dis has significantly lower temperature than in \pdis, implying that on an average, each source-destination pair observes significantly lower traffic volume in \dis when compared to \pdis.

% \paragraphb{Impact of cache size} % discussed in section 3.
%\begin{figure}[t]
%\centering
%\subfigure{
%\includegraphics[width = 3.0in]{img/rmem_numflows}
%\label{fig:rmem_numflows}
%}
%\subfigure{
%\includegraphics[width = 3.0in]{img/rmem_trafficvolume}
%\label{fig:rmem_trafvol}
%}
%\caption{Flow count (above) and traffic volume (below) with varying fraction of memory in the local cache.}
%\label{fig:rmem}
% %\end{figure}

% \paragraphb{Impact of remote versus local} 
% \rqc{Peter's graphs showing performance of local disk vs remote memory}

% \rc{what do these mean?}
 %   
%
% \subsubsection{Implications}
% Our characterization of flows in \dis allows us to draw two conclusions. First, given that \dis traffic is dominated by latency-sensitive short flows and that the number of flows increases dramatically, centralized solutions to flow scheduling may not be practical. Such solutions, if deployed in \dis, would need to dramatically lower short-flow latency as well as be able to scale up to cope with larger numbers of flows. Second, future protocols should be designed with our flow characteristics in mind --- namely, protocols should not rely on the elephant-mice conjecture for performance, and should be able to handle a more homogeneous flow size distribution. \rqc{more implications?}

% \begin{itemize}[leftmargin=*]
% 	\itemsep0em
% 	\item Traffic dominated by latency-sensitive short flows --- centralized flow schedulers may be inefficient
% 	\item Number of flows increased --- centralized solutions may see more scalability issues
% 	\item \rc{slated for removal} Flow arrival times too small --- centralized solutions inefficient
% 	\item More homogeneity in flow sizes --- design of protocols
% 	\item Traffic volume more uniformly distributed across short and long flows --- design of protocols
% \end{itemize}


% \subsubsection{Implications}
% \label{ssec:implications}
% \rc{Is this a valid implication?}
% Overall, while we observed in \S\ref{sec:requirements} that low latencies are important for performance, we observe here that high bandwidths are not necessary to serve the volume of traffic injected into the network. Rather, we envision networks in \dis being provisioned to meet \emph{latency} requirements, and being overprovisioned from a peak traffic volume perspective as a result. \rqc{more implications?} 

%\begin{itemize}[leftmargin=*]
%	\itemsep0em
%	\item Traffic volume --- do not need very high bandwidth networks
%	\item Spatial distribution --- traffic not so local
%	\item Temporal distribution --- do not need to design networks for peak traffic?
%\end{itemize}	

%Our range hashing works as follows: 
%\begin{enumerate}
%\item the address space for each node is split evenly into (number of nodes) contiguous ranges --- memory space split into 5 and disk space split into 3
%\item each range is mapped to one disaggregated node
%\item this is done for all five ec2 machines
%\item Using a range hashing algorithm allows for sequential address accesses to be pipelined for higher throughput. 
%\item We leave a more thorough exploration of this particular design knob to future work.
%\end{enumerate}

%
% \begin{figure}
% 	\centering
% 	\begin{tikzpicture}[xscale=0.5, yscale=0.35]

% 	\draw[very thick, black, <->] (-1, 12.25) to [out=45, in=135] (5, 12.25);

% 	\draw[thick, fill=white] (-3, 5) rectangle (1, 11); 
% 	\draw[thick, fill=white] (-2, 12) rectangle (0, 10); 
% 	\draw (-1, 11) node {\small{CPU}};
% 	% \draw (-1, 10.5) node {\small{Handler}};
	
% 	\draw[thick, fill=blue] (-2.75, 7.5) rectangle (-0.75, 8.5);
% 	\draw[thick, fill=green] (-2.75, 5.5) rectangle (-0.75, 7.5);
% 	\draw (-1.75, 8) node {\small{LM}};
% %	\draw (-2.75, 7.5) -- (-0.75, 7.5);
% 	\draw (-1.75, 6.5) node {\small{RM}};
% %	\draw (-2.75, 6.5) -- (-0.75, 6.5);
% %	\draw (-1.75, 6) node {\small{K$\to$O}};

% %	\draw[thick] (-0.25, 5.5) rectangle (0.75, 8.5);
% 	\draw[thick, fill=gray] (-0.25, 8.5) rectangle (0.75, 7.5);
% 	\draw[thick, fill=gray] (-0.25, 5.5) rectangle (0.75, 6.5);
% 	\draw (0.25, 7) node {\small{$\dots$}};

% 	\draw[very thick, black, dashed, <->] (-1.5, 10) -- (-1.75, 8.5);
% 	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (0.25, 8.5);
% 	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (-0.45, 6);

% 	\draw (2, 6.5) node {\Large{$\dots$}};

% 		\draw[thick, fill=white] (3, 5) rectangle (7, 11); 
% 		\draw[thick, fill=white] (4, 12) rectangle (6, 10); 
% 		\draw (5, 11) node {\small{CPU}};
% 		% \draw (5, 10.5) node {\small{Handler}};

% 		\draw[thick, fill=green] (3.25, 5.5) rectangle (5.25, 7.5);
% 		\draw[thick, fill=blue] (3.25, 7.5) rectangle (5.25, 8.5);
% 		\draw (4.25, 8) node {\small{LM}};
% 		\draw (4.25, 6.5) node {\small{RM}};

% 	%	\draw[thick] (-0.25, 5.5) rectangle (0.75, 8.5);
% 		\draw[thick, fill=gray] (5.75, 8.5) rectangle (6.75, 7.5);
% 		\draw[thick, fill=gray] (5.75, 5.5) rectangle (6.75, 6.5);
% 		\draw (6.25, 7) node {\small{$\dots$}};

% 		\draw[very thick, black, dashed, <->] (4.5, 10) -- (4.25, 8.5);
% 		\draw[very thick, black, dashed, <->] (5.5, 10) -- (6.25, 8.5);
% 		\draw[very thick, black, dashed, <->] (5.5, 10) -- (5.55, 6);

% 	\end{tikzpicture}
% 	    \caption{\small{We run real-world applications on a $5$-node Amazon EC2 cluster and emulate disaggregated architecture as follows. The memory accesses are captured into ``local cache'' accesses and ``remote memory'' accesses using an in-house implementation of a special instrumentation tool (SIT) described in \S\ref{sec:workloads}. The local disk accesses are captured using the {\tt blktrace} utility. Finally, all remote memory and disk accesses are captured using {\tt TCPdump}.}}
% 	\label{fig:system2}
% \end{figure}
%

%However, this would require changes in the application, and possibly in network elements at the end hosts. To avoid such complications, we simply assume that each remote memory access and disk access constitutes a flow in disaggregated datacenter.

% \paragraphb{Limitations}
% Our instrumentation approach has two significant limitations. First, access to memory and disk accesses is only possible at page and block granularity, respectively. While we cannot explore other points in this design space, as discussed in \S\ref{sec:summary} page and block level accesses in \dis are a reasonable granularity at which to structure remote accesses in \dis. Second, accesses caused by remote flows and accesses that originated locally are indistinguishable, and thus we use a heuristic to assign disk flows to remote CPU nodes. However, we note this limitation affects only the assignment of flows to source-destination pairs, not the amount of traffic observed.

%\begin{enumerate}
%\item Our instrumentation only allows access to memory and disk accesses at page and block granularity, respectively. While as discussed earlier page-level accesses may be desirable in \dis for locality, we were unable to explore other points in the design space.
%\item Our instrumentation cannot distinguish between accesses caused by remote flows and accesses that originated locally. We are forced to use a heuristic to assign disk flows to remote CPU nodes.
%\end{enumerate}

%
%
% \begin{figure}
%   \centering
%     \includegraphics[width = 2.5in]{img/graph2_numflows} 
%   \caption{\small{Number of flows in \dis and \pdis. Note that the y-axis is log-scaled.}}
%   \label{fig:nof}
% \end{figure}
%
%
%Figure~\ref{fig:vof} shows that the number of bytes in \dis is more uniformly distributed across short and long flows when compared to \pdis. There are two main reasons for this -- the memory flows dominating the network flows (thus having more bytes in short flows), and the long flows having been distributed across the network leading to reduction in \#bytes in long flows \rc{$\gets$ not sure about this}
%
% \begin{figure}
%   \centering
%   \subfigure{
%     \includegraphics[width = 2.3in]{img/graph4_interdist_dis} 
% 	\label{fig:asdd}
%   }
% %\hspace{0.05in}
%   \subfigure{
%     \includegraphics[width = 2.3in]{img/graph4_interdist_pdis}
% 	\label{fig:sdds}
%   }
%   \caption{\small{Flow arrival time distribution in \dis and \pdis. All the applications go into one figure: six lines in the CDF, one for each application. The same thing for \pdis. \rc{if figure stays look at graphlab going to 1}}}
%   \label{fig:fat}
% \end{figure}
%
%
%
% \begin{figure}
%   \centering
%     \includegraphics[width = 2.5in]{img/graph5_trafficvolume} 
%   \caption{\small{Traffic volume in \dis and \pdis. Note that the y-axis is log-scaled.}}
%   \label{fig:vol}
% \end{figure}
%
% \begin{figure}[t]
%   \centering
%   \subfigure{
%     \includegraphics[width = 2.3in]{img/graph7_temporaltraffic_dis_terasort} 
% 	\label{fig:asdd}
%   }
% %\hspace{0.05in}
%   \subfigure{
%     \includegraphics[width = 2.3in]{img/graph7_temporaltraffic_pdis_terasort}
% 	\label{fig:sdds}
%   }
%   \caption{\small{Temporal traffic distribution in \dis and \pdis for one of the applications. The trace was divided into 100ms timeslots, and flows were assigned to slots based on their start time. The traffic volume in these timeslots was then normalized to determine a value in bytes per second. Note the y-axis is log-scale.}}
%   \label{fig:td}
% \end{figure}
%