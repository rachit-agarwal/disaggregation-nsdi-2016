\section{\dis Network Requirements}
\label{sec:requirements}
We start by evaluating the latency and bandwidth requirements from the network such that the application-level performance in \dis is comparable to that in server-centric architectures. Overall, our results show that:

\begin{itemize}[leftmargin=*]
	\itemsep0em
	\item CPU blades having local cache is necessary to ensure reasonable performance, given the increased latency to access remote memory. However, $25\%$ of main memory serving as local cache suffices.
	\item Applications require non-trivial latency performance ($5$--$10\mu$s) from the network. In contrast, $40$Gbps access link capacity suffices. Moreover, a good reason to consider higher link capacity is not to handle higher traffic volume, but to reduce the transmission time at the end-hosts.  
    \item Ignoring congestion, the scale of disaggregation (rack versus datacenter scale) has little impact on application-level performance. \rc{$\gets$ right now, we have this results in S5 using simulations for rack and datacenter scale disagg; how should we inject latencies to make this point?}
\end{itemize}

\noindent
We start by describing our evaluation methodology (\S\ref{ssec:rmethod}), discuss our results (\S\ref{ssec:rr}) and finally outline a number of implications of our findings (\S\ref{ssec:rtt}). 

%
\begin{figure*}

  \centering
  \subfigure{
    \includegraphics[height = 1.5in]{img/vary_latency_bw.eps}  
  }
%\hspace{0.05in}
  \subfigure{
    \includegraphics[height = 1.5in]{img/vary_latency_bw_spark.eps}
  }
  \caption{\small{\rqc{The main figure for varying latency and bandwidth. One bar graph, one set of bars for each application; one bar for each configuration.}}}
  \label{fig:latb}
\end{figure*}
%
\subsection{Methodology}
\label{ssec:rmethod}
Resource disaggregation can impact application-level performance due to increased latency for main memory accesses and for disk accesses.

For memory accesses, there are two main sources of performance penalty. First, software overhead for trap and page eviction. Depending on the page eviction algorithm used, existing research prototypes report this per-page overhead to be roughly in the range of $2$--$6\mu$s~\cite{x1, x2, x3}. However, this overhead can be reduced to sub-microseconds with faster CPUs and software optimization~\cite{y1, y2, y3}, making this overhead insignificant. The second source of performance penalty is the page transfer time over the network. We focus on the second overhead since, in comparison to the first overhead, the network transfer times could be significant. 

Most applications today perform disk accesses at significantly coarser granularity than memory accesses (reading and writing of large data blocks). Most disk accesses, thus, constitute long flows and are bottlenecked is disk throughput capacity. Since network bandwidth is significantly larger than disk throughput capacity~\cite{z1, z2}, remote disk accesses in \dis are likely to continue to be bottlenecked by disk throughput (similar to server-centric architecture). Hence, we ignore the impact of increased latency for disk flows on application-level performance.

\paragraphb{Emulating remote memory} 
We emulate CPU-memory disaggregation using an in-house implementation of a light-weight special instrumentation tool (SIT) that runs atop existing servers. SIT implements a special swap device that is backed by physical memory rather than a disk. Specifically, the main memory is logically partitioned into ``local cache'' and ``remote memory'' (with a configurable fraction of main memory being the local cache). SIT intercepts all page faults, or memory access requests into the part of main memory identified as remote, and records the time and memory addresses accessed by the application. Given our assumption of remote memory access granularity being a page size, the accessed memory addresses are translated into page-level requests (number of pages requested). 

SIT intercepting all ``remote memory'' flows allows us to inject artificial delays to emulate resource disaggregation. We inject delays due to:

\begin{itemize}[leftmargin=*]
	\itemsep0em
	\item Network end-to-end latency, which is fixed for all requests. We vary end-to-end latency across experiments.
	\item Transmission time for requests, which depends both on the access link bandwidth as well as number of pages accessed in the request. For a given request, we insert (\#pages $\times$ page-size)/(bandwidth) amount of latency.
\end{itemize}

\noindent
Using the above methodology, each blade essentially experiences a disaggregated network with a constant end-to-end latency, and a fixed transmission time for a given request. The above methodology captures the queueing delays due to congestion in the network within the network latency; however, the potential effects due to congestion may be different across requests. We capture these effects in the next two sections (including the effects of disk traffic in the network).

\paragraphb{Applications}
\rc{We need to say something about generality of applications}
%We measure relative performance on the basis of throughput or request completion time as compared to the zero-delay case.

%
\begin{table}
	\centering
	\caption{\small{Workloads and applications used in our evaluation. \rc{Describe data sizes for each application; and the datasets}}}
	\label{tab:workloads}
	\vspace{0.1in}
  \begin{tabular}{c|c|c}
	  \hline
		\textbf{Application} & \textbf{Application} & \textbf{System}\\
		\textbf{domain} & \textbf{} & \textbf{}\\\hline \hline
    Batch & WordCount & Hadoop\\\hline
    Batch & WordCount & Spark\\\hline
    Batch & TeraSort & Hadoop\\\hline
    Batch & Collaborative & GraphLab\\
     & Filtering & \\\hline
    Point Queries & Key-value store & Memcached\\\hline
    % Point Queries & Search & ElasticSearch & PS\\\hline
    % Stream Processing & Wordcount & Storm & S\\\hline
    \hline
\end{tabular}
\end{table}

%
\cut{
\begin{figure}
	\centering
	\begin{tikzpicture}[xscale=0.6, yscale=0.35]

	\draw[thick, fill=white] (-3, 5) rectangle (1, 13); 
	\draw[thick, fill=white] (-2, 14) rectangle (0, 12); 
	\draw (-1, 13) node {\small{CPU}};
	% \draw (-1, 10.5) node {\small{Handler}};
	
	\draw[thick, fill=cyan] (-3.75, 9.9) rectangle (1.75, 11.1); 
	\draw (-1, 10.5) node {\small{SIT (Latency Injection)}};
			
	\draw[thick, fill=blue] (-2.75, 7.5) rectangle (-0.75, 8.5);
	\draw[thick, fill=green] (-2.75, 5.5) rectangle (-0.75, 7.5);
	\draw (-1.75, 8) node {\small{LM}};
%	\draw (-2.75, 7.5) -- (-0.75, 7.5);
	\draw (-1.75, 6.5) node {\small{RM}};
%	\draw (-2.75, 6.5) -- (-0.75, 6.5);
%	\draw (-1.75, 6) node {\small{K$\to$O}};

%	\draw[thick] (-0.25, 5.5) rectangle (0.75, 8.5);
	\draw[thick, fill=gray] (-0.25, 8.5) rectangle (0.75, 7.5);
	\draw[thick, fill=gray] (-0.25, 5.5) rectangle (0.75, 6.5);
	\draw (0.25, 7) node {\small{$\dots$}};

	\draw[very thick, black, dashed, <->] (-1.5, 10) -- (-1.75, 8.5);
	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (0.25, 8.5);
	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (-0.45, 6);

	\draw[very thick, black, dashed, <->] (-1.5, 11) -- (-1.5, 12);
	\draw[very thick, black, dashed, <->] (-0.5, 11) -- (-0.5, 12);
	\draw[very thick, black, dashed, <->] (-0.5, 11) -- (-0.5, 12);

	\end{tikzpicture}
	    \caption{\small{We run real-world applications on a $5$-node Amazon EC2 cluster. To emulate end-to-end network latency, we inject artificial latencies for all ``remote memory'' and ``remote disk'' accesses and measure the impact of this latency to the application-level performance. \rc{SIT representation imprecise}}}
	\label{fig:system1}
\end{figure}}
%
\subsection{Requirements}
\label{ssec:rr}

Figure~\ref{fig:latb} shows the application layer performance for six different combinations of network bandwidth and latency configurations. These results, as earlier, were obtained by setting the local cache to be $30\%$ of the working memory of each application. We discuss the results in-depth below. 

\paragraphb{$25\%$ local cache is sufficient}

\paragraphb{$40$Gbps, $5\mu$s is sufficient}
We start by observing that given a network with $40$Gbps access link bandwidth, and end-to-end delay of $5\mu$s, the application-level performance in \dis can be brought down to within $5\%$ of that in \pdis. While $100$Gbps network bandwidth does not provide significant benefits over the $40$Gbps case, reducing the latency down to $1\mu$s can lead to applications observing essentially no performance degradation. In the hindsight, this is not surprising given our results from \S\ref{sec:workloads}, where we established that the network traffic volume does not increase significantly in \dis compared to \pdis, and that the network flows are dominated by short (latency-sensitive) memory access flows. \rc{$\gets$ needs more concrete intuition; remote memory faster than local disk; applications heavily pipelined; CPU bottleneck?}

%\paragraphb{Benefits of remote memory}
%First, given sufficient network bandwidth and small network latencies, use of remote memory can drastically improve application performance compared to traditional disk-based swap. Since the working set size is hard to predict in advance, memory tends to be highly over-provisioned in datacenter servers to prevent thrashing. Disaggregated remote memory can reduce this waste by providing an elastic memory capacity pooled at the datacenter scale. 

%
\begin{figure}
  \centering
    \includegraphics[width = 3.5in]{img/fix_bw_vary_latency.eps} 
  \caption{\small{\rqc{Fix bandwidth to 40Gbps. Vary latency from 1us to some large number. conclude that end-to-end latency has significant impact}}}
  \label{fig:impl}
\end{figure}
%
\paragraphb{Reducing latency more important than increasing bandwidth}
Second, low latency is more important than high bandwidth. The $100$Gbps bandwidth did not provide any significant improvement over the $40$Gbps link. In contrast, $10\mu$s round-trip latency causes noticeable performance degradation, as compared to the $1\mu$s case.

%
\begin{figure}
  \centering
    \includegraphics[width = 3.5in]{img/fix_latency_vary_bw.eps} 
  \caption{\small{\rqc{Fix end-to-end latency to 5us. Vary bandwidth from 10Gbps to 100Gbps. conclude that bandwidth does not have any significant impact after 40Gbps.}}}
  \label{fig:impb}
\end{figure}
%


%
\begin{figure}
  \centering
    \includegraphics[width = 3.5in]{img/vary_remote_mem.eps} 
  \caption{\small{\rqc{Fix latency at 5us, bandwidth at 40Gbps, Vary local memory from 0\% to 90\% of the total memory of the EC2 instance}}}
  \label{fig:impb}
\end{figure}
%


\subsection{Implications}
\label{ssec:rtt}
\sr{
Outline: 
\begin{itemize}
\item we start by evaluating the basic latency and BW requirements that the network must provide in order to offer acceptable application performance. Start with methodology, then results, then implications. 
\item Methodology: (1) we focus on the impact of disaggregating memory on application performance; ignore disk for now (say why), (2) approach: each blade sees the network as a pipe with constant latency and BW  and we inject parameters; this ignores the potential effects due to congestion but we'll capture these in the next two sections (including the effects of disk traffic hitting the net). 
Gory details of how we measure.
\item Clearly, biggest impact will come from amount of local memory available. So we start by evaluating the impact of that. Show graph. Conclude 30\% (25\%??). From here on, unless stated otherwise, we're running with 30\%
\item Next, inject different combinations of latency and BW. Conclude 5 (or 5-10?) and 40. 
\item Implications (\#1) BW not a problem; latency is non-trivial. Let's consider feasibility. 
\item Feasibility of latency: `unavoidable' costs: tx delay and prop delay. Note that tx delay is non-trivial. Three implications of this: (\#2) can't afford any/much queueing delay (i.e., no congestion). Will return to evaluate the feasibilty of zero/low queueing delay in S5, and (\#3) Ignoring congestion, rack vs. dc-scale not such a big diff. Only alters the prop delay component. So perhaps dc-scale is OK. Finally, (\#4), a good reason to consider 100G (which is already available today at both switch ports and server NICs but not commonly deployed) is, not because we need the capacity, but to reduce (tx) latency. 
\end{itemize}}

\begin{itemize}
	\item Implications? Why are current technology trends favorable?
	\item Feasibility: Existing solutions and technology trends
\end{itemize}