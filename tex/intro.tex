\section{Introduction}
\label{sec:intro}
Existing datacenters are based on a server-centric architecture, where each server aggregates a small amount of the resources (CPU, memory, storage, interconnects) needed for computing tasks. While the server-centric architecture has been a tremendous success, recent industry trends suggest a paradigm shift --- a {\em disaggregated} architecture where the resources are decoupled. The Disaggregated Data Center (DDC), thus, comprises of a pool of resources, with each resource type built as a standalone resource blade and a datacenter-wide network interconnects all resource blades. Examples abound already --- Intel RSA, HP ``the machine'', Facebook's disaggregated rack, as well as research prototypes (Firebox, soNUMA).

Resource disaggregation is beneficial along several dimensions. First, disaggregation makes computing hardware more modular, enabling the technology for each individual resource to evolve independently. Indeed, as new technologies evolve or need for specialized computing needs arise, physically decoupling the resources alleviates the burdensome process of integration, server factor form planning, and motherboard designs. Second, resource disaggregation also provides a fine-grained control over provisioning, sharing and efficiently utilizing individual resources. Finally, resource disaggregation allows overcoming the technology barriers (imbalance between memory and CPU capacity, power dissipation issues, etc), potentially enabling new technological advances. 

What is the barrier to scaling disaggregation? The greatest burden of disaggregation falls on the network. Indeed, the inter-resource communication that used to be contained within a server is now spread across the datacenter-wide fabric. This not only increases the load on the network but makes low latency communication critical. The key enabling or blocking factor to disaggregation will be the network. 

Unfortunately, the network support required for resource disaggregation is, at best, poorly understood. The existing deployments of disaggregated datacenters are either small scale or proprietary, with little details available publicly~\cite{x1, x2, x3, x4}. \sr{need citations \ldots} 


This paper takes a step in building an understanding of the requirements and subsequent challenges to be resolved to enable network support for disaggregated datacenters. Our approach is workload-driven; we use five common, yet diverse, workloads to answer three questions through a combination of experiments and simulation: 

\begin{itemize}[leftmargin=*]
	\itemsep0em
		\item What support will applications need from the network, in terms of latency and bandwidth, to maintain the application-level performance within $10\%$ of server-centric architecture?
	\item How will the network traffic change in DDC? What are the important design parameters that impact traffic patterns?
    \item Can existing transport protocols meet the above requirements? 
\end{itemize}

\noindent
The workloads used in our study span a wide range of applications, including batch processing jobs from Hadoop and Spark, point queries from Memcached~\cite{memcached} and ElasticSearch~\cite{elastic} and streaming applications from Storm~\cite{storm}. We also study the impact of several design parameters for DDC, including scale of disaggregation (rack-scale or datacenter-scale), size of local cache on CPU blades, data placement strategies across disaggregated I/O blades, etc. Our key findings are:

\begin{itemize}[leftmargin=*]
	\itemsep0em
		\item Independent of the design parameters in DDC, applications can achieve performance within $10\%$ of current server-centric architectures using a full-bisection bandwidth network with $40$Gbps bandwidth capacity, and an end-to-end latency of $5\mu$s. These requirements ignore the optimizations that disaggregation enables. Nevertheless, it highlights the challenge: while $40$Gbps (or even $100$Gbps) is feasible with existing technology, the key to resource disaggregation is to achieve low end-to-end latency between resource blades. 
	    \item While the design parameters significantly impact DDC traffic characteristics, in general, many common tenets that have guided datacenter design to date no longer hold. As expected, DDC traffic volume increases; however, we observe: (a) relatively more homogeneous spatial and temporal distribution of traffic; (b) concentration of traffic within a smaller range of flow sizes; and (c) more homogeneous traffic volume distribution between short flows and long flows. 
        \item Existing state-of-the-art transport protocols~\cite{pfabric} may be sufficient to meet the application requirements for DDC traffic. Intuitively, short memory flows dominating the DDC traffic combined with a more homogeneous spatial and temporal distribution allows existing protocols to achieve the desired performance. \rqc{room for improvement?}
\end{itemize}

\noindent
\sr{Add a list of caveats/disclaimers as follows\ldots}
\begin{itemize} 
\item our results are based on the workloads we study; not comprehensive
\item we focus on net design, ignore many systems questions; may still well turn out that the latter matters more; in this sense one might view our study as seeing whether the network can `get out of the way’ \cite{};
\item because our study is forward looking, many aspects of the overall context we’re considering don’t exist yet so we must make assumptions - e.g., data layout, how resources blades are organized, etc.  We make what we believe are sensible choices (and state these explicitly) but our results are dependent on these. More experience is needed to confirm these hold.
\end{itemize} 

 \sr{To date however,} there is no consensus on the granularity at which resource disaggregation will happen --- at the rack-scale, pod-scale, or an extreme of datacenter scale. 

