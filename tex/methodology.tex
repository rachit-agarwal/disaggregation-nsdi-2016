\section{Methodology Detail}

\subsection{Setup}
\begin{enumerate}
\item 1 master with 5 worker nodes on ec2
    \begin{enumerate}
    \item m3.2xlarge in us-east-1a
    \item 30GB RAM
    \item 8 core CPU (Intel Xeon E5-2670 v2, 2494.022 MHz, 25M cache)
    \item 2 * 80 GB SSD
    \item Virtual Private Cluster is used to guarantee network bandwidth
    \end{enumerate}
\item applications:
    \begin{enumerate}
    \item wordcount
        \begin{enumerate}
        \item Compute framework: Spark
        \item Trace: Wikipedia raw trace with XML metadata
        \item Language: Scala and Java
        \item Input Size: 125 GB total
        \item Input from/Output to: HDFS
        \item For both Spark and Hadoop, we reconfig the HDFS such that both of the 80GB hard drive are used for storage.
        \end{enumerate}
    \item wordcount-hadoop
        \begin{enumerate}
        \item Compute framework: Hadoop
        \item Trace: Wikipedia raw trace with XML metadata
        \item Language: Java
        \item Input Size: 125 GB total
        \item Input from/output to: HDFS
        \end{enumerate}
    \item terasort
        \begin{enumerate}
        \item Compute framework: Hadoop
        \item Trace: Random 100 byte key generated by hadoop
        \item Language: Java
        \item Input Size: 125 GB total
        \item Input from/ Output to: HDFS
        \item Hadoop is reconfigured so that both of the SSD drives are used for map-reduce spilling (temp data between map phase and reduce phase)
        \end{enumerate}
    \item graphlab
        \begin{enumerate}
        \item Compute framework: Graphlab
        \item App: collaborative filtering (Alternative Least Square)
        \item Language: C++
        \item Trace: Netflix film rating trace
        \item Input trace: 25GB per machine, the same graph is replicated to all 5 worker nodes
        \item Input from/Output to: local disk
        \end{enumerate}
    \item memcached
        \begin{enumerate}
        \item Workload: YCSB Benchmark B.
        \item 2.6M 10K records each machine
        \item 13M operations on 8 worker threads each machine
        \item load balancing: random hash load balancing.
        \item Language: java
        \item Input are generated by CPU and directly inserted to memcached
        \item We measure average access latency as the main metric
        \end{enumerate}
    \end{enumerate}
\item Measurement approach
    \begin{enumerate}
    \item SIT
        \begin{enumerate}
        \item Intercepts all swap requests from the OS, and we can inject latency to/log each of the request and
        \item Bakup by local memory
        \item By default, we configure 25\% of the total memory to be local (30 * 0.25 = 7.5G) and the remaining 22.5GB of memory are considered as remote. 
        \item Latency injection:
            \begin{enumerate}
            \item Parameters: $Bandwidth$ and $Constant\_Latency$
            \item For each swap request, it contains a list of pages that the OS wants to swap out / swap in
            \item At the beginning of each request, SIT waits $Constant\_Latency$ amount of time by entering a spinning loop.
            \item After that, SIT iterate through the list of pages in the request and does the actual swapping.
            \item For each page swap action, SIT again waits for ${Page\_Size / Bandwidth$ time by entering a spinning loop
            \item So for a swap request that contains $N$ pages, the $Total\_Injected\_Latency = Constant\_Latency + Page\_Size / Bandwidth $
            \item The default bandwidth and latency injected are 40Gbps and 5us
            \end{enumerate}
        \item Swap trace logging
            \begin{enumerate}
            \item Each time a swap action is performed, a record is generated and stored inside SIT's memory. The record contains the <record id> <utc timestamp, us> <page location> <length, pages> <batch seq no>
            \item The length entry is always 1 because of some kernel legacy problem.
            \item Since each request contains multiple pages to swap, <batch seq no>, which indicates the order of that page swap in the request.
            \item To fetch the records from the kernel module to user space, we use the /proc file system. An external process is periodically polling records from the /proc file.
            \item Sangjin told me that we can get the Page Frame Number (PFN) of each swap request. That could be useful information as well. 
            \end{enumerate}
        \end{enumerate}

    \item blktrace
        \begin{enumerate}
        \item We use blktrace to monitor /dev/xvda, where / is mounted, and the two 80Gb storage SSDs (/dev/xvdb /dev/xvdc), which contain the HDFS data folder and HDFS temp file folder. For applications that does not use HDFS, their input and output data are also located in these two SSDs.
        \item For each disk IO access, blktrace generates multiple records, such as ``Request Queued'', ``Issued to Driver'', ``Request Completed''. We do filtering at the logging phase and only keep ``Issued to driver'' records.
        \item At each run, each CPU core will generate a file that only contains its own disk access records. These files will be processed together when we use blkparse to process them.
        \end{enumerate}

    \item tcpdump
        \begin{enumerate}
        \item We run tcpdump to capture all network traffic and combine packet records to flow records online.
        \item We identify a flow by its 5-tuple: (src ip, src port, dst ip, dst port, protocol)
        \item If two packets have the same 5-tuple, they are considered to be in the same flow.
        \item For each flow we log the start time, end time and the length of the flow (in bytes), as well as the 5 tuple.
        \end{enumerate}
    
    \end{enumerate}

\end{enumerate}

\subsection{Making Flows}
A trace comprises of 5 types of files:
\begin{enumerate}
\item memory trace
    \begin{enumerate}
    \item one file per ec2 machine.
    \item format: \texttt{<record id> <utc timestamp, us> <page location> <length, pages> <batch seq no>}
    \item Each line processed independently.
    \item time is converted to seconds by dividing by 1e6.
    \item if access is a read, timestamp is negative. if access is a write, timestamp is positive (read/write bit is stored, then ts = abs(ts))
    \item <page location> represents the page's address in the swap device
    \item length of the flow = flow length, pages * size of page
    \item size of page is always 4096 bytes.
    \end{enumerate}
\item metadata file
    \begin{enumerate}
    \item gives the utc timestamp (per node) of the start of the disk trace.
    \item second file gives the percentage of remote memory in the experiment and other info (across all experiment nodes). Not used in trace mapping, only for background info.
    \end{enumerate}
\item disk trace
    \begin{enumerate}
    \item processing takes the disk trace start (given by metadata file)
    \item there are multiple files per node (one per cpu per ec2 machine). The blkparse utility will combine the files for the same machine together.
    \item blkparse output is filtered to remove the following applications: python|tcpdump|blktrace|cat|swap|bash|sh|auditd since these applications represent activity from the instrumentation itself rather than the application
    \item blkparse output is piped through get\_disk\_io.py, which parses and has the following output: \texttt{<field identifier> <timestamp> <field identifier> <addr> <field identifier> <length> <unused> <unused> <field identifier> <rw> <unused> <unused>}
    \item time of disk flow = time offset (from metadata file) + record timestamp
    \end{enumerate}
\item nic trace
    \begin{enumerate}
    \item format: \texttt{<time> <src> <dst> <size, bytes>}
    \item src, dst are in terms of an internal identifier. Mapping from <identifier> -> {0,1,2,3,4} is in metadata file.
    \end{enumerate}
\end{enumerate}

\subsubsection{Remapping Disk Flow Sources}
Some disk flows represent remote disk reads. So, some other machine initiated a network connection, which then resulted in a disk read taking place. Therefore, the resulting disk flow should be assigned to the remote machine rather than the machine on which the read occurred.
To fix this, we use the following heuristic:
\begin{enumerate}
\item Only disk \textbf{reads} have their endpoint changed - disk writes are assumed to be local
\item Per node, filter nic flows to only include flows where the \textbf{destination} is that node
\item Sort these flows by time, add to a queue
\item sort disk flows by time.
\item for disk flow in all disk reads:
    \begin{enumerate}
    \item step 1: if (disk flow time is between start and end of current nic flow and nic flow has remaining size) assign disk flow to the nic flow (disk flow destination = nic flow source, nic flow size -= disk flow size)
    \item otherwise pop the next nic flow and repeat step 1
    \item if these are no more nic flows, all remaining disk flows are assumed to be local reads.
    \end{enumerate}
\end{enumerate}

\subsubsection{Combining Parsed Flows}
\begin{enumerate}
\item only disk flows are combined.
\item Time adjustment: the smallest time $t_{start}$ across all flows for all nodes is found. For each flow f, $f_{start} -= t_{start}$
\item Address Mapping
    \begin{enumerate}
    \item Host ids for ``disaggregated'' nodes are assigned as follows (assuming 5 original ec2 machines): 0, 1, 2, 3, 4 CPU, 5, 6, 7, 8, 9 Memory, 10, 11, 12 Disk.
    \item so numNodes = 5 for memory, 3 for disk
    \item Per node, for each of (CPU, Memory):
        \begin{enumerate}
        \item addrRange = max(flow addresses) - min(flow addresses)
        \item hostForFlow(f) = floor($(f_{addr} / \text{addrRange}) * numNodes$)
        \item nodeId = (5 if memory, 10 if disk) + hostForFlow(flow)
        \end{enumerate}
    \end{enumerate}
\item Direction: If \textbf{read}, flow is \textbf{from Disk/Memory TO CPU}. If \textbf{write}, flow is \textbf{from CPU TO Disk/Memory}.
\item Collapsing Flows: for batching, we collapse flows together
    \begin{enumerate}
    \item group flows by type - memory or disk
    \item memory flows \textbf{are not collapsed, only disk flows are}
    \item group to-be-collapsed flows by (source, destination) pairs
    \item sort each group by time
    \item start at beginning. if (flow b start <= flow a ``collapsing time (= start for first flow in group)'' + 50 us)
        \begin{enumerate}
        \item flow a length += flow b length
        \item flow a ``collapsing time'' = flow b start time
        \end{enumerate}
    \item repeat that while there are flows to combine. If the next flow cannot be combined, emit the so-far-combined flow as one flow, and start a new aggregation.
    \end{enumerate}
\item After the preceding transformations are done, \texttt{globalListOfFlows.append(flow)}
\end{enumerate}

Finally, sort all flows by time and output trace.
Graphing script reads this trace and plots various graphs.

\subsection{Simulator}
Two versions: ``verbatim'' and ``scale-up''
\begin{enumerate}
\item ``verbatim'' read flows directly from outputted trace and inject them into the simulator, running phost and pfabric. There are 13 randomly chosen nodes active.
\item ``scale-up'' read trace and construct a 13 by 13 traffic matrix. Each entry in the matrix contains 2 CDFs: flow size distribution and interarrival time distribution. Simulator takes this matrix and generates traffic for 144 nodes as follows:
    \begin{enumerate}
    \item For each node, select one of 13 nodes to be the ``sender profile'' at random.
    \item Pick 12 other nodes randomly from the 144 node topology
    \item for each of those 12, assign a receiver profile from the corresponding sender column of the traffic matrix.
    \item traffic from that sender to that receiver will now be drawn from the appropriate size, interarrival CDFs.
    \end{enumerate}
\end{enumerate}