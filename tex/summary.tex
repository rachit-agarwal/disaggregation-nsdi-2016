\vspace{-0.1in}
\section{Disaggregated Datacenters}
\vspace{-0.05in}
\label{sec:summary}

The high-level idea behind a disaggregated datacenter is illustrated in Figure~\ref{fig:dc}.
A \dis comprises standalone hardware “blades” for each resource type, %including CPUs, memory, storage, and network interfaces as well as specialized components (GPUs, various ASIC accelerators, etc.). Those resource blades are
interconnected by a unified network fabric. Understanding the nature of this network fabric is our focus in this paper.
However, before delving into network designs, we outline our assumptions regarding the hardware (\S\ref{ssec:hardware}) and system (\S\ref{ssec:system}) architecture in \dis. 


\vspace{-0.25in}
\subsection{Hardware Architecture}
\vspace{-0.05in}
\label{ssec:hardware}
The hardware design we assume for \dis follows that of previous studies and prototypes~\cite{ddcHwDesign1, ddcHwDesign2, ddcHwDesign3, rsa, hptm, fdr, sonuma, firebox}. We summarize these here and refer the reader to~\cite{ddcHwDesign1, ddcHwDesign2, ddcHwDesign3} for detailed hardware designs.% for disaggregated blades. 


\paragraphb{Standalone per-resource blades}
With some simplification, we can view each blade as comprised of the resource component in question with a direct interface to the network fabric. 
One exception to this strict decoupling are compute blades. Since memory access from CPUs must run at very high speed (see Table~\ref{tab:tech}), each compute blade is assumed to retain some amount of \emph{local} memory that acts as a cache for remote memory~\cite{ddcHwDesign1, ddcHwDesign2}. Thus, disaggregating memory can be viewed as expanding the memory hierarchy to include a remote level. While remote memory may be allocated to any CPU in the datacenter, local memory is dedicated to its co-located CPU. We evaluate how local memory impacts performance in~\S\ref{sec:requirements}. 
%is necessary to ensure reasonable performance given the increased latency to access remote memory.


\paragraphb{Cache coherence domain is limited to a single compute blade}
As we elaborate on below, we assume that the abstraction of a Virtual Machine (VM) is retained in \dis and that, similar to today, each VM is limited to a single compute blade (which may house one or more CPU `sockets'). This has the important implication that cache coherence domains are constrained to 
a single physical blade and hence CPU-to-CPU cache coherence traffic does not hit the network fabric.
This assumption is necessary because an external network fabric is unlikely to support the latency and bandwidth requirements for inter-CPU cache coherence (Table~\ref{tab:tech}).

\cut{
\paragraphb{Each CPU blade hosts a small local cache}
As proposed in previous work~\cite{ddcHwDesign1}, we assume that each CPU blade retains a small amount of local memory (rather than completely disaggregating the memory) that acts as a cache for remote memory. While remote memory may be allocated to any CPU in the datacenter, local memory is dedicated to its co-located CPUs. \rqc{As we shall see, the assumption of local memory is necessary to ensure reasonable performance given the increased latency to access remote memory.}
}

\paragraphb{Virtualizing resources}
Each resource blade must support virtualization of its resources. 
We thus assume that each memory blade has a controller ASIC (or a lightweight processor) that implements address translation between a remote CPU's view of its address space and the addressing used internally within a blade, as described in~\cite{ddcHwDesign1}.
For IO blades: many IO device controllers now support virtualization via PCIe SR-IOV or MR-IOV features and the same can be leveraged to virtualize IO resources in \dis. \sr{need citations for features}
We note that while the implementation of such blades may require some additional new hardware, it requires no change to existing components such as CPUs, memory modules, or storage devices themselves.


%{\footnote{\sr{WHY?\rc{We should comment on metadata management here}}}}. 

%
\begin{table}
  \centering
  \small
  \begin{tabular}{l|c|c}
		\textbf{Communication} & \textbf{Latency (ns)} & \textbf{Bandwidth (Gbps)}\\\hline
	\hline
    CPU -- CPU & $10$ & $500$\\\hline
    CPU -- Memory & $20$ & $500$\\\hline
    CPU -- $40$G NIC & $10^3$ & $40$\\\hline
    CPU -- Disk (SSD) & $10^4$ & $5$\\\hline
    CPU -- Disk (HDD) & $10^6$ & $1$\\\hline
    \hline
  \end{tabular}
  \vspace{0.1in}
  \caption{\small{Typical latency and peak bandwidth requirements within a traditional server. Numbers vary between hardware.}}
  \label{tab:tech}
\end{table}
%
\vspace{-0.1in}
\subsection{System Architecture}
\vspace{-0.05in}
\label{ssec:system}
%We now outline our assumptions regarding the system architecture in \dis. 
In contrast to our assumptions regarding hardware which we based on existing  prototypes, we have less to guide us on the systems front. We thus make the following assumptions, which we believe are reasonable: 


\paragraphb{VM as an abstraction}
In current datacenters, resource allocation and scheduling is centered around the abstraction of a VM: 
operators provision VMs to aggregate slices of hardware resources within a server, and schedulers place jobs across VMs. 
While not strictly necessary, we note that the VM model can still be useful in \dis. In particular, continuing with the abstraction of a VM will allow existing software infrastructure --- i.e., hypervisors, operating systems, datacenter middleware, and applications --- to be reused with little or no modification. Thus in this paper we assume that computational resources are still aggregated to form VMs, although now the resources assigned to a VM come from distributed hardware blades. Given a VM abstraction, we assign resources to VMs differently based on the \emph{scope} of disaggregation that we assume: for rack-scale disaggregation, a VM is assigned resources from within a single rack while, for datacenter-scale disaggregation, a VM is assigned resources from anywhere in the datacenter.


\paragraphb{Hardware organization} 
We assume that resources are organized in racks as in today's datacenters. 
We assume a `mixed' organization in which each rack hosts a mix of different types of resource blades, as opposed to a `segregated' organization in which a rack is populated with a single type of resource (e.g., all memory blades). The rationale for a mixed organization is twofold. 
First, it leads to a more uniform communication pattern which should simplify network design. 
Second, a mixed organization permits optimizations that aim to localize communication; e.g., co-locating a VM within a rack, which would not be possible with a segregated organization. 
%\sr{clear?}\sh{to me yes, but should be in \S\ref{ssec:hardware}?}
%\rqc{(in some proportion that isn't important)} \rc{Shouldn't this go to the previous subsection?}

%The scheduler allocates resources by assigning resource blades (or a part of), configuring the resultant resource and address space assignments at the selected blades, and configuring the network that interconnects these blades. Depending of the scale of disaggregation, each rack may contain different types of resource blades and the scheduler may optimize for locality when it allocates resources to a job. The best layout of resource blades and the corresponding scheduler optimizations is a topic for future exploration.

\paragraphb{Page-level remote memory access}
We assume that CPU blades access remote memory at the granularity of a page (4KB in x86). In traditional servers , the typical memory access between CPU and DRAM occurs in the unit of a cache-line size (64B in x86); however, page-level access has been shown to better exploit spatial locality in common memory access patterns and hence amortizes the round-trip latency more effectively in the context of \dis ~\cite{ddcHwDesign1}. In addition, page-level access requires little or no modification to the virtual memory subsystems of hypervisors or operating systems, and is completely transparent to user-level applications. We further assume that those remotely accessed pages are not shared by multiple VMs at a given time, in order to not introduce cache coherence traffic across the network.

\paragraphb{Block-level distributed data placement}
We assume that applications in \dis read and write large files at the granularity of ``blocks'' (again 4KB in x86, as most OSes have the same block size as pages) with consecutive blocks uniformly distributed across different disk blades. This assumption is partially motivated by existing distributed file systems (\eg, HDFS) and also enables better load balancing and fault tolerance. % because this spreads load. 

\vspace{-0.1in}
\subsection{Design knobs}
\vspace{-0.05in}
\label{ssec:knobs}
Given the above assumptions, we are left with two key system design choices that we treat as ``knobs'' in our study: {\em the amount of local memory on compute blades} and  {\em the scope of disaggregation} (e.g., rack-, pod- or datacenter-scale). We will explore how varying these knobs impacts the network requirements and traffic characteristics in \dis.

The remainder of this paper is organized as follows: we start by analyzing network  bandwidth and latency requirements (\S\ref{sec:requirements}), then study how disaggregation impacts network traffic characteristics (\S\ref{sec:workloads}), and the suitability of existing network protocols for disaggregated workloads and requirements (\S\ref{sec:existing}). We end with a discussion of related work and open questions (\S\ref{sec:discussion}).

%--------------------------------------------------
% END OF USEFUL STUFF
%-----------------------------


% \cut{
% \begin{itemize}[leftmargin=*]
% 	\itemsep0em
% 	\item {\bf Size of local cache:} how does the size of local cache on CPU blades affect the network requirements and traffic characteristics for \dis?
% 	\item {\bf Scale of disaggregation:} whether disaggregation is performed at the rack-scale, pod-scale or the entire datacenter scale?
% 	\item {\bf Data placement:} whether or not locality in data placement and access essential for \dis? \sr{This isn't a knob really since we're just picking one model?} \rc{$\gets$ Yes, unlikely that we will be able to evaluate multiple data placement strategies.}
% %	\item {\bf page size and access granularity:} \sr{ditto above?} ??
% \end{itemize} }

%\sr{Perhaps simplify how this is presented (though specific text can be reused) with slightly massaged org as follows:
%
%\begin{itemize}
%\item S2.2) System-level assumptions: Unlike hardware design, we have less to guide us on this front. Hence, we make the following which we believe are reasonable:
%\begin{itemize}
%\item VM as abstraction. Rationale: This seems like the natural starting point since it allows seamless transition for legacy apps. (copy from below and hotnets)
%\item Hardware organization: we assume racks (as today) where each rack has a mix of different types of resource blades (in some proportion that isn't important). This is in contrast to (say) a rack full of only disk blades, another with mem blades, etc. Rationale: leads to a more uniform architecture, hence simplifies the network design. Also, means hardware organization is independent of scope of disaggr (i.e., only scheduler needs to change to support different scopes).
%\item Distributed data placement: because this spreads load. We see no compelling reason to do otherwise (right?)
%\item Page-level memory/storage access (copy rationale from HotNets paper
%\end{itemize}
%\end{itemize}
%\begin{itemize}
%\item (S2.3 (or part of 2.2)) This leaves us with two key system design choices that we treat as knobs: local cache size and the scale of disaggregation. We consider different possibilities for both in this paper.
%\item On to next section (i.e., no methodology) \ldots 
%\end{itemize}
%
%}


% \cut{\sr{not needed with new org above?} \st{As briefly outlined in \S\ref{sec:intro}, the scale at which resource disaggregation will happen is, at best, unclear. The goal of this paper is not to advocate any particular architecture for disaggregation datacenters. Nevertheless, we believe that some of the architectural aspects of disaggregated datacenters are inevitable for reasons of feasibility, performance and scalability. }

% \sr{I'd replace para below by just saying that: in this section we explain our assumptions w.r.t. hardware (S2.1), system architecture (S2.2) and our  design `knobs' (S2.3). } 
% \st{We start this section by elaborating on our interpretation of a disaggregated datacenter, along with a discussion of some necessary architectural design decisions and the rationale behind these decisions (\S\ref{ssec:arch}). We then make some important assumptions regarding the usage model, implementation and execution of computing frameworks on this architecture (\S\ref{ssec:assumptions}) that allow us to explore the three questions outlined in \S\ref{sec:intro}. Finally, in \S\ref{ssec:summary}, we give a high-level overview of the experimental and simulation setup used in our study.}}

%
% \begin{figure*}
% 	\centering
% 	\label{fig:ddcIllustration}
% 	\subfigure[Workload Characterization (\S\ref{sec:workloads})] {	
% 	\begin{tikzpicture}[xscale=0.5, yscale=0.35]

% 	\draw[very thick, black, <->] (-1, 12.25) to [out=45, in=135] (5, 12.25);

% 	\draw[thick, fill=white] (-3, 5) rectangle (1, 11); 
% 	\draw[thick, fill=white] (-2, 12) rectangle (0, 10); 
% 	\draw (-1, 11) node {\small{CPU}};
% 	% \draw (-1, 10.5) node {\small{Handler}};
	
% 	\draw[thick, fill=blue] (-2.75, 7.5) rectangle (-0.75, 8.5);
% 	\draw[thick, fill=green] (-2.75, 5.5) rectangle (-0.75, 7.5);
% 	\draw (-1.75, 8) node {\small{LM}};
% %	\draw (-2.75, 7.5) -- (-0.75, 7.5);
% 	\draw (-1.75, 6.5) node {\small{RM}};
% %	\draw (-2.75, 6.5) -- (-0.75, 6.5);
% %	\draw (-1.75, 6) node {\small{K$\to$O}};

% %	\draw[thick] (-0.25, 5.5) rectangle (0.75, 8.5);
% 	\draw[thick, fill=gray] (-0.25, 8.5) rectangle (0.75, 7.5);
% 	\draw[thick, fill=gray] (-0.25, 5.5) rectangle (0.75, 6.5);
% 	\draw (0.25, 7) node {\small{$\dots$}};

% 	\draw[very thick, black, dashed, <->] (-1.5, 10) -- (-1.75, 8.5);
% 	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (0.25, 8.5);
% 	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (-0.45, 6);

% 	\draw (2, 6.5) node {\Large{$\dots$}};

% 		\draw[thick, fill=white] (3, 5) rectangle (7, 11); 
% 		\draw[thick, fill=white] (4, 12) rectangle (6, 10); 
% 		\draw (5, 11) node {\small{CPU}};
% 		% \draw (5, 10.5) node {\small{Handler}};

% 		\draw[thick, fill=green] (3.25, 5.5) rectangle (5.25, 7.5);
% 		\draw[thick, fill=blue] (3.25, 7.5) rectangle (5.25, 8.5);
% 		\draw (4.25, 8) node {\small{LM}};
% 		\draw (4.25, 6.5) node {\small{RM}};

% 	%	\draw[thick] (-0.25, 5.5) rectangle (0.75, 8.5);
% 		\draw[thick, fill=gray] (5.75, 8.5) rectangle (6.75, 7.5);
% 		\draw[thick, fill=gray] (5.75, 5.5) rectangle (6.75, 6.5);
% 		\draw (6.25, 7) node {\small{$\dots$}};

% 		\draw[very thick, black, dashed, <->] (4.5, 10) -- (4.25, 8.5);
% 		\draw[very thick, black, dashed, <->] (5.5, 10) -- (6.25, 8.5);
% 		\draw[very thick, black, dashed, <->] (5.5, 10) -- (5.55, 6);



% 			% \draw[thick, black] (8, 4.75) -- (8.1, 4.5);
% 			% \draw[thick, black] (8.1, 4.5) -- (9.75, 4.5);
% 			% \draw[thick, black] (9.75, 4.5) to [in=90, out=0] (10, 4.25);
% 			% \draw[thick, black] (10.25, 4.5) to [in=90, out=180] (10, 4.25);
% 			% \draw[thick, black] (10.25, 4.5) -- (11.9, 4.5);
% 			% \draw[thick, black] (12, 4.75) -- (11.9, 4.5);
% 			% \draw (10, 3.75) node {\tt SuffixStore};


% 			% 	\draw[thick, fill=white] (13, 5) rectangle (17, 11);
% 			% 	\draw[thick, fill=white] (14, 12) rectangle (16, 10);
% 			% 	\draw (15, 11.5) node {\small{CPU}};
% 			% 	% \draw (15, 10.5) node {\small{Handler}};
% 			%
% 			% 	\draw[thick] (13.25, 5.5) rectangle (15.25, 8.5);
% 			% 	\draw (14.25, 8) node {\small{P$\to$M}};
% 			% 	\draw (13.25, 7.5) -- (15.25, 7.5);
% 			% 	\draw (14.25, 7) node {\small{O$\to$P}};
% 			% 	\draw (13.25, 6.5) -- (15.25, 6.5);
% 			% 	\draw (14.25, 6) node {\small{K$\to$O}};
% 			%
% 			% %	\draw[thick] (-0.25, 5.5) rectangle (0.75, 8.5);
% 			% 	\draw[thick, fill=gray] (15.75, 8.5) rectangle (16.75, 7.5);
% 			% 	\draw[thick, fill=gray] (15.75, 5.5) rectangle (16.75, 6.5);
% 			% 	\draw (16.25, 7) node {\small{$\dots$}};
% 			%
% 			% 	\draw[very thick, black, dashed, <->] (14.5, 10) -- (14.25, 8.5);
% 			% 	\draw[very thick, black, dashed, <->] (15.5, 10) -- (16.25, 8.5);
% 			% 	\draw[very thick, black, dashed, <->] (15.5, 10) -- (15.55, 6);

% 				% \draw[thick, black] (13, 4.75) -- (13.1, 4.5);
% 				% \draw[thick, black] (13.1, 4.5) -- (14.75, 4.5);
% 				% \draw[thick, black] (14.75, 4.5) to [in=90, out=0] (15, 4.25);
% 				% \draw[thick, black] (15.25, 4.5) to [in=90, out=180] (15, 4.25);
% 				% \draw[thick, black] (15.25, 4.5) -- (16.9, 4.5);
% 				% \draw[thick, black] (17, 4.75) -- (16.9, 4.5);
% 				% \draw (15, 3.75) node {\tt LogStore};


% 	\end{tikzpicture}
% 	}
% %  	\end{minipage}
% %  	\begin{minipage}[c]{0.19\textwidth}
% % 	    \caption{\small{{\bf Methodology Overview:} We run real-world applications on a $5$-node Amazon EC2 cluster and emulate disaggregated architecture as follows. The memory accesses are captured into ``local cache'' accesses and ``remote memory'' accesses using an in-house implementation of a special instrumentation tool (SIT) described in \S\ref{sec:workloads}. The local disk accesses are captured using the {\tt blktrace} utility. Finally, all remote memory and disk accesses are captured using {\tt TCPdump}.}}
% % 	\label{fig:system}
% % %	\end{minipage}
% % \end{figure}
% % %
% % %
% % %
% % \begin{figure}
% %  \begin{minipage}{0.33\textwidth}
% % 	\centering
% 	\hspace{0.1in}
% 	\subfigure[Understanding application requirements (\S\ref{sec:requirements})] {	
% 	\begin{tikzpicture}[xscale=0.6, yscale=0.35]

% 	% \draw[thick, fill=white] (4, 14) rectangle (10, 19);
% 	% \draw (7, 18.25) node {{Coordinator}};
% 	% \draw[thick] (4.25, 14.5) rectangle (9.75, 17.5);
% 	% \draw (7, 17) node {\small{Partition$\to$Machine}};
% 	% \draw (4.25, 16.5) -- (9.75, 16.5);
% 	% \draw (7, 16) node {\small{Offset$\to$Partition}};
% 	% \draw (4.25, 15.5) -- (9.75, 15.5);
% 	% \draw (7, 15) node {\small{Key$\to$Offset} \footnotesize{(NoSQL only)}};
% 	%
% 	% \draw[very thick, dashed, gray, <->] (7, 13.75) -- (0, 12.5);
% 	% \draw[very thick, dashed, gray, <->] (7, 13.75) -- (5, 12.5);
% 	% \draw[very thick, dashed, gray, <->] (7, 13.75) -- (10, 12.5);
% 	% \draw[very thick, dashed, gray, <->] (7, 13.75) -- (15, 12.5);
% 	%
% 	% \draw[very thick, black, <->] (-1, 12.25) to [out=45, in=135] (5, 12.25);
% 	% \draw[very thick, black, ->] (0.75, 13.25) -- (4, 12.25);
% 	% \draw[very thick, black, <-] (-1, 12.25) to [out=45, in=180] (0.5, 13.75);
% 	% \draw[very thick, black, ->] (0.5, 13.75) -- (9, 12.25);
% 	% \draw[very thick, black, <-] (-1, 12.25) to [out=45, in=180] (0.25, 14.25);
% 	% \draw[very thick, black, ->] (0.25, 14.25) -- (14, 12.25);

% 	% \draw[very thick, black, <->] (-0.5, 16.5) -- (-1.25, 12);
% 	% \draw (-0.5, 17.5) node {\small{\tt search(string str)}};

% 	\draw[thick, fill=white] (-3, 5) rectangle (1, 13); 
% 	\draw[thick, fill=white] (-2, 14) rectangle (0, 12); 
% 	\draw (-1, 13) node {\small{CPU}};
% 	% \draw (-1, 10.5) node {\small{Handler}};
	
% 	\draw[thick, fill=cyan] (-3.75, 9.9) rectangle (1.75, 11.1); 
% 	\draw (-1, 10.5) node {\small{SIT (Latency Injection)}};
			
% 	\draw[thick, fill=blue] (-2.75, 7.5) rectangle (-0.75, 8.5);
% 	\draw[thick, fill=green] (-2.75, 5.5) rectangle (-0.75, 7.5);
% 	\draw (-1.75, 8) node {\small{LM}};
% %	\draw (-2.75, 7.5) -- (-0.75, 7.5);
% 	\draw (-1.75, 6.5) node {\small{RM}};
% %	\draw (-2.75, 6.5) -- (-0.75, 6.5);
% %	\draw (-1.75, 6) node {\small{K$\to$O}};

% %	\draw[thick] (-0.25, 5.5) rectangle (0.75, 8.5);
% 	\draw[thick, fill=gray] (-0.25, 8.5) rectangle (0.75, 7.5);
% 	\draw[thick, fill=gray] (-0.25, 5.5) rectangle (0.75, 6.5);
% 	\draw (0.25, 7) node {\small{$\dots$}};

% 	\draw[very thick, black, dashed, <->] (-1.5, 10) -- (-1.75, 8.5);
% 	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (0.25, 8.5);
% 	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (-0.45, 6);

% 	\draw[very thick, black, dashed, <->] (-1.5, 11) -- (-1.5, 12);
% 	\draw[very thick, black, dashed, <->] (-0.5, 11) -- (-0.5, 12);
% 	\draw[very thick, black, dashed, <->] (-0.5, 11) -- (-0.5, 12);

% 	\end{tikzpicture}
% 	}
% %  	\end{minipage}\hfill
% %  	\begin{minipage}[c]{0.19\textwidth}
% % 	    \caption{\small{{\bf Methodology Overview:} We run real-world applications on a $5$-node Amazon EC2 cluster. To emulate end-to-end network latency, we inject artificial latencies for all ``remote memory'' and ``remote disk'' accesses and measure the impact of this latency to the application-level performance.}}
% % 	\label{fig:system}
% %	\end{minipage}
% % \end{figure}
% % %
% % %
% % \begin{figure}
% %  \begin{minipage}{0.33\textwidth}
% % 	\centering
% 	\hspace{0.1in}
% 	\subfigure[Exploring sufficiency of existing solutions (\S\ref{sec:existing})] {	
% 	\begin{tikzpicture}[xscale=0.6, yscale=0.35]

% 	% \draw[thick, fill=white] (4, 14) rectangle (10, 19);
% 	% \draw (7, 18.25) node {{Coordinator}};
% 	% \draw[thick] (4.25, 14.5) rectangle (9.75, 17.5);
% 	% \draw (7, 17) node {\small{Partition$\to$Machine}};
% 	% \draw (4.25, 16.5) -- (9.75, 16.5);
% 	% \draw (7, 16) node {\small{Offset$\to$Partition}};
% 	% \draw (4.25, 15.5) -- (9.75, 15.5);
% 	% \draw (7, 15) node {\small{Key$\to$Offset} \footnotesize{(NoSQL only)}};
% 	%
% 	% \draw[very thick, dashed, gray, <->] (7, 13.75) -- (0, 12.5);
% 	% \draw[very thick, dashed, gray, <->] (7, 13.75) -- (5, 12.5);
% 	% \draw[very thick, dashed, gray, <->] (7, 13.75) -- (10, 12.5);
% 	% \draw[very thick, dashed, gray, <->] (7, 13.75) -- (15, 12.5);
% 	%
% 	% \draw[very thick, black, <->] (-1, 12.25) to [out=45, in=135] (5, 12.25);
% 	% \draw[very thick, black, ->] (0.75, 13.25) -- (4, 12.25);
% 	% \draw[very thick, black, <-] (-1, 12.25) to [out=45, in=180] (0.5, 13.75);
% 	% \draw[very thick, black, ->] (0.5, 13.75) -- (9, 12.25);
% 	% \draw[very thick, black, <-] (-1, 12.25) to [out=45, in=180] (0.25, 14.25);
% 	% \draw[very thick, black, ->] (0.25, 14.25) -- (14, 12.25);

% 	% \draw[very thick, black, <->] (-0.5, 16.5) -- (-1.25, 12);
% 	% \draw (-0.5, 17.5) node {\small{\tt search(string str)}};

% 	\draw[thick, fill=white] (-8, 5) rectangle (-5, 9); 
% 	\draw[thick, fill=white] (-8, 9.75) rectangle (-5, 11.25); 
% 	\draw[thick, fill=white] (-8, 12) rectangle (-5, 14); 
% 	\draw (-6.5, 8.5) node {\small{Network}};
% 	\draw (-6.5, 7.5) node {\small{Simulator}};
% 	\draw (-6.5, 6.5) node {\small{(Individual}};
% 	\draw (-6.5, 5.5) node {\small{Flow delays)}};
% 	\draw (-6.5, 10.5) node {\small{Scale up}};
% 	\draw (-6.5, 13.5) node {\small{Flows from}};
% 	\draw (-6.5, 12.5) node {\small{Step (a)}};
% 	\draw[thick, black, ->] (-6.5, 12) -- (-6.5, 11.25);
% 	\draw[thick, black, ->] (-6.5, 9.75) -- (-6.5, 9);
% 	\draw[thick, black, -] (-5, 7) -- (-4.5, 7);
% 	\draw[thick, black, -] (-4.5, 7) -- (-4.5, 10.5);
% 	\draw[thick, black, ->] (-4.5, 10.5) -- (-3.75, 10.5);

% 	\draw[thick, fill=white] (-3, 5) rectangle (1, 13); 
% 	\draw[thick, fill=white] (-2, 14) rectangle (0, 12); 
% 	\draw (-1, 13) node {\small{CPU}};
% 	% \draw (-1, 10.5) node {\small{Handler}};
	
% 	\draw[thick, fill=cyan] (-3.75, 9.9) rectangle (1.75, 11.1); 
% 	\draw (-1, 10.5) node {\small{SIT (Latency Injection)}};
			
% 	\draw[thick, fill=blue] (-2.75, 7.5) rectangle (-0.75, 8.5);
% 	\draw[thick, fill=green] (-2.75, 5.5) rectangle (-0.75, 7.5);
% 	\draw (-1.75, 8) node {\small{LM}};
% %	\draw (-2.75, 7.5) -- (-0.75, 7.5);
% 	\draw (-1.75, 6.5) node {\small{RM}};
% %	\draw (-2.75, 6.5) -- (-0.75, 6.5);
% %	\draw (-1.75, 6) node {\small{K$\to$O}};

% %	\draw[thick] (-0.25, 5.5) rectangle (0.75, 8.5);
% 	\draw[thick, fill=gray] (-0.25, 8.5) rectangle (0.75, 7.5);
% 	\draw[thick, fill=gray] (-0.25, 5.5) rectangle (0.75, 6.5);
% 	\draw (0.25, 7) node {\small{$\dots$}};

% 	\draw[very thick, black, dashed, <->] (-1.5, 10) -- (-1.75, 8.5);
% 	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (0.25, 8.5);
% 	\draw[very thick, black, dashed, <->] (-0.5, 10) -- (-0.45, 6);

% 	\draw[very thick, black, dashed, <->] (-1.5, 11) -- (-1.5, 12);
% 	\draw[very thick, black, dashed, <->] (-0.5, 11) -- (-0.5, 12);
% 	\draw[very thick, black, dashed, <->] (-0.5, 11) -- (-0.5, 12);

% 	\end{tikzpicture}
% 	}
% %  	\end{minipage}
% %  	\begin{minipage}[c]{0.19\textwidth}
% 	    \caption{\small{(left) {\bf Workload Characterization:} We run real-world applications on a $5$-node Amazon EC2 cluster and emulate disaggregated architecture as follows. The memory accesses are captured into ``local cache'' accesses and ``remote memory'' accesses using an in-house implementation of a special instrumentation tool (SIT) described in \S\ref{sec:workloads}. The local disk accesses are captured using the {\tt blktrace} utility. Finally, all remote memory and disk accesses are captured using {\tt TCPdump}. (center) {\bf Understanding application requirements:} We run real-world applications on a $5$-node Amazon EC2 cluster. To emulate end-to-end network latency, we inject artificial latencies for all ``remote memory'' and ``remote disk'' accesses and measure the impact of this latency to the application-level performance. (right) {\bf Exploring sufficiency of existing solutions:} Same as Step 2, but the latencies injected for each flow are now a result of network simulation results. \rc{SIT representation imprecise}}}
% 	\label{fig:system}
% %	\end{minipage}
% \end{figure*}
%

%\paragraphb{Virtualization, resource allocation and scheduling}
%The scheduler allocates resources by assigning resource blades (or a part of), configuring the resultant resource and address space assignments at the selected blades, and configuring the network that interconnects these blades. Depending of the scale of disaggregation, each rack may contain different types of resource blades and the scheduler may optimize for locality when it allocates resources to a job. The best layout of resource blades and the corresponding scheduler optimizations is a topic for future exploration.

% \paragraphb{VM as a computational unit}
% The current datacenter usage model is heavily based on the server-centric architecture. While physical servers in datacenters have evolved to server virtualization~\cite{cc} or other comparable technologies, they are still all centered around the concept of server, which aggregates slices to hardware resources within a server.
%
% In contrast, the usage model of a disaggregated datacenter does not necessarily follow the same approach; since computation, storage, and I/O functions can be completely disseminated across the datacenter, we do not need to restrict our usage model within the VM-oriented architecture. However, we note that the VM model can be still useful,
%
% \paragraphb{Local and remote memory}
% While disaggregation of I/O devices is relatively straightforward as discussed above, memory disaggregation brings a set of new challenges in terms of performance. Since memory access from CPUs must run at very high speed, similar to prior work, we assume that each CPU blade retains some amount of local memory that acts as a cache for remote memory; this disaggregating memory can be viewer as expanding the memory hierarchy to include a remote level.
%
% \paragraphb{Page-level remote memory access}
% We assume that CPU blades access remote memory at the page granularity (4KB in x86) over the fabric. While typical memory access between CPU and DRAM in traditional servers occurs in the unit of cache-line size (64B in x86), it is known that page level access better exploits spatial locality in common memory access patters and amortized the routing-trip latency more effectively. In addition, page-level access requires little or no modification to the virtual memory subsystems of hypervisor or operating system, and it is completely transparent to user-level applications. We further assume that those remotely accessed pages are not shared by multiple VMs at a given time, in order to not introduce cache coherence traffic across the network.
%
% \paragraphb{Block-level remote storage access}
% We assume that CPU blades access remote storage at the block granularity (4KB in x86) over the fabric. This is the storage access granularity between CPU and current storage devices (disk, SSD, etc) in traditional servers.

% \cut{
% \subsection{Methodology Overview}
% \sr{skip this section and insert methodology inline with each section as relevant? It's too hard to follow (or weigh) at this point.}

% Figure~\ref{fig:system} gives a high-level overview of our methodology to explore the three questions outlined in \S\ref{sec:intro}. We provide details for each of the three steps in respective sections, but note that the requirements from the network fabric are heavily dependent on the underlying application. To that end, we explore a wide variety of applications varying from batch processing (long running background jobs), point queries (user facing short jobs) and stream processing (real-time data analytics), as outlined in Table~\ref{tab:workloads}. }
%Each of these applications typically have very different data read/write patterns, which will translate to very different traffic patterns in disaggregated datacenters. In fact, within these application types, each particular system and/or algorithm may have different patterns. To that end, we use all six different workloads across these three applications in our evaluation. These workloads are described in Table~\ref{tab:workloads}.

% \begin{table}
% 	\centering
% 	\caption{\small{Workloads and applications used in our evaluation. \rc{Describe data sizes for each application; and the datasets}}}
% 	\label{tab:workloads}
% 	\vspace{0.1in}
%   \begin{tabular}{c|c|c}
% 	  \hline
% 		\textbf{Application} & \textbf{Application} & \textbf{System}\\
% 		\textbf{domain} & \textbf{} & \textbf{}\\\hline \hline
%     Batch & WordCount & Hadoop\\\hline
%     Batch & WordCount & Spark\\\hline
%     Batch & TeraSort & Hadoop\\\hline
%     Batch & Collaborative & GraphLab\\
%      & Filtering & \\\hline
%     Point Queries & Key-value store & Memcached\\\hline
%     % Point Queries & Search & ElasticSearch & PS\\\hline
%     % Stream Processing & Wordcount & Storm & S\\\hline
%     \hline
% \end{tabular}
% \end{table}

%
% \subsection{Summary of key findings}
% \label{ssec:summary}
%
% \begin{itemize}
% 	\item Key Findings
% 	\begin{itemize}
% 		\item Network traffic
% 			\begin{itemize}
% 				\item no elephant/mice: implication?
% 				\item inter-arrival times no longer poisson: implication?
% 				\item traffic less bursty: implication?
% 				\item various design knobs for better control: implications?
% 			\end{itemize}
% 		\item Application Requirements
% 		\begin{itemize}
% 			\item 40Gbps suffices
% 			\item 5us RTT sufficient to maintain current performance
% 		\end{itemize}
% 		\item Existing Solutions enough?
% 	\end{itemize}
% \end{itemize}